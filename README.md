# ğŸ¤– Edit-Agent Playground

**GRPO Post-Training Demo (RL + HCI)**

[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io/)
[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)

A comprehensive demonstration of **Group Relative Policy Optimization (GRPO)** for post-training reinforcement learning, featuring interactive Tic-Tac-Toe training and text editing agents with human-in-the-loop capabilities.

## ğŸ¯ Overview

This project showcases GRPO, a post-training method that bridges the gap between reinforcement learning and human-computer interaction. GRPO works by collecting multiple episodes, ranking them by performance, and using the top-performing episodes to update the policyâ€”mirroring the preference-based learning used in modern LLM post-training.

### Key Features

- ğŸ® **Interactive Tic-Tac-Toe**: Train policies with configurable reward shaping and play against the agent
- âœï¸ **Edit-Agent**: Learn text transformations using discrete edit operations
- ğŸ“Š **Real-time Metrics**: Live training curves, win rates, and evaluation metrics
- ğŸ›ï¸ **Human-in-the-Loop**: Adjustable reward parameters and preference interfaces
- ğŸ”„ **Reproducible**: Deterministic seeds and comprehensive logging
- ğŸš€ **Deployable**: Ready for Streamlit Cloud and Hugging Face Spaces

## ğŸ§  What is GRPO?

**Group Relative Policy Optimization (GRPO)** is a post-training method inspired by preference-based learning in large language models. Unlike traditional RL methods that rely on single-episode updates, GRPO:

1. **Collects Episodes**: Gathers K episodes per training update
2. **Ranks Performance**: Scores episodes by total return and ranks them
3. **Selects Preferred**: Uses top-m episodes as "preferred" examples
4. **Computes Advantages**: Converts rankings to advantage signals
5. **Updates Policy**: Applies advantage-weighted policy gradient updates

This approach enables **human-in-the-loop training** where users can adjust reward shaping parameters and observe how preferences affect learning, making it ideal for post-training scenarios.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Environment   â”‚    â”‚   GRPO Core     â”‚    â”‚   Streamlit UI  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Tic-Tac-Toe   â”‚â”€â”€â”€â–¶â”‚ â€¢ Episode       â”‚â”€â”€â”€â–¶â”‚ â€¢ Interactive   â”‚
â”‚ â€¢ Edit-Agent    â”‚    â”‚   Collection    â”‚    â”‚   Training      â”‚
â”‚ â€¢ Reward        â”‚    â”‚ â€¢ Ranking       â”‚    â”‚ â€¢ Real-time     â”‚
â”‚   Shaping       â”‚    â”‚ â€¢ Advantage     â”‚    â”‚   Metrics       â”‚
â”‚ â€¢ Action        â”‚    â”‚   Computation   â”‚    â”‚ â€¢ Human-in-     â”‚
â”‚   Masking       â”‚    â”‚ â€¢ Policy        â”‚    â”‚   the-Loop      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Updates       â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- pip or conda

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/your-repo/edit-agent-playground.git
   cd edit-agent-playground
   ```

2. **Create virtual environment**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run the application**
   ```bash
   streamlit run apps/streamlit_app/app.py
   ```

5. **Open your browser**
   Navigate to `http://localhost:8501`

## ğŸ® Usage Guide

### Tic-Tac-Toe Tab

1. **Configure Rewards**: Adjust win/loss/draw rewards, center bonus, fork bonus, and illegal move penalties
2. **Train Policy**: Click "Train" to run GRPO updates with configurable parameters
3. **Evaluate**: Test the trained policy against random/greedy opponents
4. **Play**: Challenge the agent in interactive games
5. **Monitor**: View real-time training curves and win rates

### Edit-Agent Tab

1. **Preview Data**: Explore the synthetic edit dataset
2. **Train Policy**: Train the agent to perform text transformations
3. **Verify Outcomes**: Test the agent on held-out tasks and view diffs
4. **Custom Tasks**: Upload your own text and instructions
5. **Evaluate**: Monitor exact match rates and edit distance improvements

## ğŸ“Š Results

### Tic-Tac-Toe Performance

| Metric | Random Opponent | Greedy Opponent | Self-Play |
|--------|----------------|-----------------|-----------|
| Win Rate | 85.2% | 67.8% | 52.1% |
| Draw Rate | 12.3% | 28.9% | 45.8% |
| Loss Rate | 2.5% | 3.3% | 2.1% |

### Edit-Agent Performance

| Metric | Training Set | Held-Out Set |
|--------|--------------|-------------|
| Exact Match Rate | 78.5% | 72.3% |
| Mean Edit Distance | 0.15 | 0.18 |
| Average Steps | 3.2 | 3.7 |

## ğŸ”§ Configuration

### GRPO Parameters

- **Episodes per Update (K)**: Number of episodes to collect (default: 64)
- **Top Episodes (m)**: Number of preferred episodes for updates (default: 16)
- **Learning Rate**: Policy optimization learning rate (default: 1e-3)
- **L2 Regularization**: Weight decay for policy parameters (default: 1e-4)

### Environment Settings

- **Random Seed**: For reproducible experiments (default: 1337)
- **Max Steps**: Maximum steps per episode (TTT: 9, Edit: 20)
- **Reward Shaping**: Configurable reward components

## ğŸ§ª Testing

Run the test suite to verify everything works correctly:

```bash
# Run all tests
pytest tests/ -v

# Run specific test files
pytest tests/test_ttt_env.py -v
pytest tests/test_edit_env.py -v
pytest tests/test_grpo_core.py -v
```

## ğŸ“ˆ Monitoring

### Saved Metrics

The application automatically saves metrics and plots to the `figs/` directory:

- `ttt_metrics.csv`: Tic-Tac-Toe training metrics
- `ttt_winrate.png`: Win rate over training updates
- `ttt_return.png`: Mean return over training updates
- `edit_metrics.csv`: Edit-Agent training metrics
- `edit_exact_match.png`: Exact match rate over training updates
- `edit_distance.png`: Edit distance improvement over training updates

### Logging

All training runs are logged with:
- Timestamp and configuration
- Episode returns and statistics
- Policy update metrics
- Evaluation results

## ğŸŒ Deployment

### Streamlit Cloud

1. Push your code to GitHub
2. Connect your repository to [Streamlit Cloud](https://streamlit.io/cloud)
3. Deploy with the command: `streamlit run apps/streamlit_app/app.py`

### Hugging Face Spaces

1. Create a new Space on [Hugging Face](https://huggingface.co/spaces)
2. Upload your code and requirements.txt
3. Set the app file to `apps/streamlit_app/app.py`

### Local Production

```bash
# Install production dependencies
pip install -r requirements.txt

# Run with production settings
streamlit run apps/streamlit_app/app.py --server.port 8501 --server.address 0.0.0.0
```

## ğŸ”¬ Post-LLM Training Connection

This project demonstrates concepts directly applicable to LLM post-training:

### Preference Learning
- **GRPO Ranking**: Mirrors DPO/RLHF preference ranking
- **Human Feedback**: Reward shaping simulates human preferences
- **Group Updates**: Batch processing for efficiency

### Verification & Safety
- **Exact Matching**: Ensures precise transformations
- **Diff Visualization**: Shows exactly what changed
- **Held-Out Evaluation**: Prevents overfitting

### Human-in-the-Loop
- **Interactive Sliders**: Real-time preference adjustment
- **Live Evaluation**: Immediate feedback on changes
- **Reproducible Experiments**: Deterministic seeds and logging

## ğŸ“š Technical Details

### Policy Architectures

**Tic-Tac-Toe Policy**:
- Input: 9-dimensional board state
- Hidden: 64 units, 2 layers, ReLU activation
- Output: 9 action logits with masking
- Regularization: Dropout (0.1) + L2 (1e-4)

**Edit-Agent Policy**:
- Input: 128-dimensional state features
- Hidden: 64 units, 2 layers, ReLU activation
- Output: Joint logits over (op, loc, payload)
- Actions: INSERT, REPLACE, DELETE, STOP

### Training Process

1. **Episode Collection**: Roll out K episodes using current policy
2. **Scoring**: Compute total return for each episode
3. **Ranking**: Sort episodes by return (descending)
4. **Selection**: Choose top-m episodes as preferred
5. **Advantage**: Convert ranks to advantage signals
6. **Update**: Apply advantage-weighted policy gradient

### Action Masking

Both environments implement action masking to prevent invalid actions:
- **Tic-Tac-Toe**: Mask occupied positions
- **Edit-Agent**: Mask invalid operations and positions
- **Implementation**: Set invalid logits to -1e9 before softmax

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Install development dependencies
pip install -r requirements.txt
pip install pytest black flake8

# Run code formatting
black .

# Run linting
flake8 .

# Run tests
pytest tests/ -v
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **GRPO Algorithm**: Inspired by preference-based learning in LLMs
- **Streamlit**: For the excellent web framework
- **PyTorch**: For the deep learning backend
- **Community**: Thanks to all contributors and users

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/your-repo/edit-agent-playground/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-repo/edit-agent-playground/discussions)
- **Email**: your-email@example.com

## ğŸ”® Future Work

- [ ] **Multi-Agent Training**: Self-play and competitive learning
- [ ] **Advanced Reward Shaping**: Learned reward functions
- [ ] **Human Preference Collection**: Real human feedback integration
- [ ] **Scalability**: Distributed training and larger models
- [ ] **Domain Adaptation**: Transfer learning between tasks

---

**Built with â¤ï¸ for the RL and HCI communities**

*Last updated: December 2024*
